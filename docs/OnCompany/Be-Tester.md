

### 测试的价值?

写独立开发项目有感, 再整个过程中包括 
产生想法, 选题对比(调研), 开发, 页面设计, 文案布局, 机器选择, 部署架构, Fallback 机制, SEO, 运营, 
等内容, 都不是测试在本职工作中能用到的, 因此我想到, 这份持续几年的工作到底带来了什么

但仔细一想真实如此吗, 作为一个测试基本上了解一个公司产品(直接经手或者间接经手过)的 80% 的部分, 只要有心留意就能获知其几乎完整的商业流程, 对比文档, 以及实际动向能推理出这一步战略原因

**具体? 代入本次开发过程, 谈及测试对于这个过程的加持**

比较产品: 可以说, 产品只知道表象, 至少我们这里是这样的, 数据没有专门的收集任务, 只能从已有的崩溃率查看问题, 没有更好的正向迭代方向, 属于拿旧东西一直用. 主要想表达它们对于整体的了解不如测试, 虽然有一定获取数据的渠道, 但都是小问题

需求: 对于整体的了解, 因为工作涉及, 需求的评审已经要求测试参与进去, 而有一部分是运维直接回馈, 通过回溯运维需求能对这次改动有更深入的了解

开发: 
> 1.部分 lua 和 Python 代码是在部署时可见的, 就意味着随时可查, 更便利的制作出测试 tools
> 2.涉及难度原因, 开发的修改是必须要讲清楚的, 对于向领导汇报工作, 以及集体有很大帮助, 最开始不懂的时候直接依赖开发提供测试用例, 我们在后边捡漏(边界值测试), 因此对与设计逻辑有很深入的了解, 而...

运维; 虽然有了解一些内容, 比如辅助上线, 但专业内容肯定不如, 而且因风气原因, 我们的中间商身份已被蚕食差不多了



定位我所做的测试  
我和所测专业内容互相成就  
测试分为很多种, 单元测试, 集成测试 ...接口测试, 功能测试

我所测内容在公司内部成为 CDN 测试, 顾名思义 目标是以最快速 + 稳定 + 节省资源的方式 提供媒资给用户, 不过不只是大后端, 为了更好对接展示端(前端的 SDK(Ranger) ) 也需要直接, 由后方开发接手进行高度定制化, 迎合服务器胃口也由我们测试

那如何介绍呢, 总不能说 CDN 测试, 太笼统, 又不能细说, 少了听不懂, 长了不好讲


https://gemini.google.com/app/54e64521809f34d8

但最终还是归功于业余的求知, 双方互辅的得以成就

无论最终的 Project 能否成功(虽然大概率不能成功盈利), 这份经验却是最低的收获了


### 用例结构

使用 xmind 进行导入导出

```
# GSLB

## 定义用例结构

### 所属服务（GSLB）

- menu（定义下方所有功能点场景， 使用简单的语言概括便于理解具体场景）


	- redis 连接失败

		- 从 PG 连接正常

			- 不查询

				- 功能点1

		- 从 PG 连接异常

			- 不查询

				- 此场景结果与功能点1结果相同，不验证

	- redis 未命中

		- 从 PG 查询成功
		- 从 PG 查询失败

- 功能点1

	- 详细描述，条件等
	- 输入/操作步骤: 
	- 预期结果

		- 结果1
		- 结果2

-  示例（功能点1）- 用户信息查询 - Redis 异常 (连接失败/超时/其他错误) 从库 DB 正常。

	- GSLB 连接的 Redis 异常 (如网络断开、服务停止、响应超时、返回错误)，从库 DB 正常。
	- 通过请求方式调度用户策略
	- 预期结果:

		- GSLB 查询 Redis 失败
		- GSLB 不查询从库 DB。

			- GSLB 不查询主库 DB。

		- GSLB 无法获取策略， 用户分组信息未查到
		- 不依赖查询信息的策略不受影响， 比如运营商


- 注意点

	- 如果让你帮忙写测试用例, 你应输出这种 markdown 语法，子集的方式到canvas, 方便我导入导出. 一般来说用例输出应该使用中文
	- 你写的用例应该符合之前用例的写法特征
	- 除了排列格式外 - 符号和 ## header 符号外 不要使用 markdown 的其它语法符号， 比如 **** 加粗， 等
```

### 关于测试用例的思考, 人人都能写的测试用例
测试用例从哪来： 从一份设计文档中， 以及开发讲述中扣出测试点来， 再进化为用例
扣出测试点依赖一部分逻辑思考能力和部分对系统的熟悉，我认为在这一部分思考能力更重要，对系统的熟悉助力你判断着是否属于一个测试点

而对于用例书写则更多依赖于对系统的熟悉，越熟悉就越全面，越能自主判断测试点重要性，比如：根据之前经验这个地方有过错误， 需要着重测试

还有，用例框架也较为重要。关于用例复用: 个人认为还是要看类型， 纯增量的修改可能不太有复用机会， 而如果是重构优化部分内容则会用到一些，我就会基于原来的一份进行增删用例

### Unclass




#### 日志问题,
不同函数日志上下文, 有的以 service_id 为过滤条件, 有的能从 mc 过滤到, 有的只能从函数 name 过滤, 增加查询难度
比如 spm send_task 最终要 handle 时已经跨文件了, 最终是否处理需要 handle 时打印, 而其没有加

#### 从哪里推导出问题:
1.最终现象不为期望, 这是最简单的方式, 可是一般需要佐证, 帮助判断条件是正常的, 以降低误判概率, 自己也能再反省(比如提供处理时日志, 配置的图片, 数据等)
2.找出具体日志, 进行分辨后交给开发. 这里有个问题: (期间跨服务器短时间内有多个操作不好找日志) 推荐之后开发写代码检查加上对日志纠察, 甚至给每一次处理传递 唯一 ID, 方便快速找到附近日志

suggestion2 :
获取代码
python 页面的, 代码权限, 现在只有一个可执行文件以及配置
SPM 部分因为 openresty 本身就暴露了代码
而core 等网元代码太复杂, 这方面还是由开发查更快

原因:
1.方便确认逻辑. 为什么不直接找开发: 简单的一些设计可能自己打开文件看下比找开发问更快
更便于分析问题







掉进了增量设计测试的漩涡, 为了测试而测试的感觉, 项目结束后马不停蹄下一个, 反思还要抽出时间, 比如抽出时间想想这个 
汇报



收集问题: 
大部分问题都不是结论直接发到群里的, 而是开发开完后直接解决, 即 没有公布答案, 正好方便猜想, 完成后再询问, 即使不记得也能提供思路
很大一部分 core 都没收集, 太琐碎, 也没有足够时间

最重要的, 增加测试对于线上问题参与度

1.由测试接手有哪些难度, 对于现状进行分析
> 对于现状进行分析, 即 如果线上出现问题, 运维直接找开发协助解决, 而不是测试.
	1.开发时间周期短, 测试时间长, 因此开发有更长的处理问题时长(虽然有解决 bug), 造成了依赖
	2.对问题依旧不够熟悉, 对全局理解度不够, 对于线上数据的了解


  > 分工方面, 更熟悉系统 比如: "url 错了 看下". 能直接判断谁的问题.
	开发长期跟进线上, 而非测试,比如: 二级缓存
	  对于线上数据的了解, 比如: 手机数据等


2.由测试接手如何实施 

经过 线上问题记录处理 尝试了解问题类型, 归类问题....  示例: 回看页面的问题和开发一起定位了
1.像 bug 一样, 运维提给测试, 测试确认后转给开发, 协同跟进, 而像 core 问题一类的可以直接给开发, 
就是创建一个新的提问题方式, 如果依旧使用之前的方式可能短时间强调后有帮助, 但时间长了又恢复了, 可能说很多次还没有改善这一个结局.









